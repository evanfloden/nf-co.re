---
title: Guidelines
subtitle: Guidelines and requirements for nf-core pipelines.
---

## Requirements for nf-core pipelines

The aim of nf-core is to have _standardised_ best-practice pipelines.
To ensure this standardisation, we maintain a set of guidelines which all _nf-core_
pipelines must adhere to.

> If you're thinking of adding a new pipeline to nf-core, please read the documentation
> about [adding a new pipeline](adding_pipelines.md).

## Workflow specificity

The nf-core community was founded to allow different groups to collaborate on
pipelines instead of reinventing the same workflows in each institute.
As such, different pipelines should not overlap one another too much:
there should only be a single pipeline for a given data + analysis type.
However, if the _purpose_ of the pipeline tasks and results are different, then this should be a separate pipeline.

If you would like to use a different set of tools to do a comparable analysis, then this should be
added to the existing pipeline instead of creating something new.

## Workflow size

We aim to have a _"not too big, not too small"_ rule with nf-core pipelines.
This is deliberately fuzzy, but as a rule of thumb workflows should contain at
least three processes and be simple enough to run that a new user
can realistically run the pipeline after spending ten minutes reading the docs.
Pipelines should be general enough to be of use to multiple groups and research
projects, but comprehensive enough to cover most steps in a primary analysis.

## Use the template

All nf-core pipelines must be built using the nf-core template.
Ideally, they should be _started_ using the `nf-core create` command which
makes a new git repository and the initial commits and branches.
This is to ensure that the automated sync process that keeps all nf-core
pipelines up to date can work. See the [sync docs](/developers/sync) for details.

## Minimum requirements

All nf-core pipelines _must_ adhere to the following:

* Be built using Nextflow
* Have an [MIT licence](https://choosealicense.com/licenses/mit/)
* Have software bundled using [Docker](https://www.docker.com/)
* Continuous integration testing
* Stable release tags
* Common pipeline structure and usage
  * Standard filenames as supplied in the template, such as `main.nf` and `docs/`
  * Use the same command line option names as other pipelines for comparable options, _e.g._ `--input` and `--genome`
* Run in a single command
  * _i.e._ Not multiple separate workflows in a single repository
  * It is ok to have workflows that use the output of _another_ nf-core pipeline as input
* Excellent documentation and GitHub repository keywords
* A responsible contact person / GitHub username
  * This will typically be the main person behind the pipeline development
  * This person should be responsible for basic maintenance and questions
* The pipeline must not have any failures in the `nf-core lint` tests
  * These tests are run by the [nf-core/tools](https://github.com/nf-core/tools) package and validate the requirements listed on this page.
  * You can see the list of tests and how to pass them on the [error codes page](https://nf-co.re/tools-docs).

## Recommended features

If possible, it's great if pipelines can also have:

* All software bundled using [bioconda](https://bioconda.github.io/)
  * Nearly all nf-core pipelines use a conda `env` script to list their software requirements.
    The pipeline Docker images are then built using this, meaning that with a single file your pipeline can support nextflow users running with conda, docker or singularity.
  * The [nf-core template](/tools#creating-a-new-workflow) comes with all required code to support this setup.
* All pipelines should attempt to use a recent reference genome draft(_ie_ `GRCh38` for human...)
* Optimised output file formats
  * Pipelines should generate `CRAM` alignment files by default, but have a `--bam` option to generate `BAM` outputs if required by the user.
* Digital object identifiers (DOIs) for easy referencing in literature
  * Typically each release should have a DOI generated by [Zenodo](https://zenodo.org/). This can be automated through linkage with the GitHub repository.
* Explicit support for running in cloud environments
  * For example, use of [AWS-iGenomes](https://ewels.github.io/AWS-iGenomes/) and aws-batch
  * The [nf-core template](/tools#creating-a-new-workflow) comes with all required code to support this setup.
* Benchmarks from running on cloud environments such as [AWS](https://aws.amazon.com/)

## Workflow name

All nf-core pipelines should be lower case and without punctuation.
This is to maximise compatibility with other platforms such as Docker Hub, which enforce such rules.
In documentation, please refer to your pipeline as `nf-core/yourpipeline`.

## Coding style

The nf-core style requirements are growing and maturing over time.
Typically, as we agree on a new standard we try to build a test for it into the `nf-core lint` command.
As such, to get a feel for what's expected, please read the [lint test error codes](https://nf-co.re/tools-docs).

However, in general, pipelines must:

* Use [config profiles](https://www.nextflow.io/docs/latest/config.html) to organise hardware-specific options
* Run with as little input as possible
  * Metadata (eg. be able to run with just FastQ files, where possible)
  * Reference files (eg. auto-generate missing reference files, where possible)
* Keep only code for the latest stable release on the main `master` branch.
  * The main development code should be kept in a branch called `dev`
  * The nf-core `dev` branch should be set as the default branch up until the first release
* Use GitHub releases and [keep a detailed changelog](https://keepachangelog.com/en/1.0.0/) file
* Follow a versioning approach, e.g. [Semantic Versioning](https://semver.org/) for your pipeline releases

## Credits and Acknowledgements

Where previous work from other pipelines / projects is used within a pipeline, the original author(s) must be properly acknowledged. Some examples and on how you could do that to make sure they feel valued:

* Send them a message via Slack and let them know that you use their work & had to change something to fit your own purpose. If in doubt, check with them to see how they would like to be acknowledged.
* Check the licence of their code and make sure you obey the rules that this licence imposes (e.g. `CC-BY` means you have to attribute the original creator).
* If you use portions of pipeline code, even if its just tiny pieces:
  * Link to the original repository and/or authors.
  * Leave existing credits and acknowledgement sections intact - there may be more than just a single author involved.
* If you find bugs / issues, report and fix them upstream in the main project.

If in doubt about what to do, ask the nf-core community about it on Slack.

## Ask the community

The instructions above are subject to interpretation and specific scenarios.
If in doubt, please ask the community for feedback on the [`#new-pipelines` Slack channel](https://nfcore.slack.com/channels/new-pipelines).
You can join the nf-core Slack [here](/join).
